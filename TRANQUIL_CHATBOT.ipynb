{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRANQUIL_CHATBOT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq3SMyX8GH2a"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA7e1YkUGQ6e"
      },
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h90-2IhZGcPa",
        "outputId": "67302938-1161-4222-c907-9c90272bf340"
      },
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
            "\r\u001b[K     |███                             | 10kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-cp37-none-any.whl size=127301 sha256=db2d05520492b2858e769b5fea3fa9eb71eef4594696e3489f75db642776f69a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax_4woihGjnV",
        "outputId": "b58dedac-d1c7-4897-c8c9-4b3ca4724b22"
      },
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tflearn -y\n",
        "!pip install tensorflow-gpu==1.15\n",
        "!pip install tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Uninstalling tflearn-0.5.0:\n",
            "  Successfully uninstalled tflearn-0.5.0\n",
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (54.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=5273d23248e5591d5c57954b66b0b6e6313b33d32ae52d3945afcf2c33862f67\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Processing /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744/tflearn-0.5.0-cp37-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1GH9aGnG_NW",
        "outputId": "891c3ae7-04a1-4ab6-a44d-e21e4095b5ca"
      },
      "source": [
        "import tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWS6QZk2HGGT"
      },
      "source": [
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9gyjKpUsHJXj",
        "outputId": "9819887f-9cad-47f9-b9ae-a7c4d1c2b1d3"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98b5d3cc-3fb5-4071-a509-ae48fc8884b7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98b5d3cc-3fb5-4071-a509-ae48fc8884b7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving intents.json to intents.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "N5if87e8HSKG",
        "outputId": "6ed542df-e72e-4cee-991f-37a589e18d57"
      },
      "source": [
        "file_name = \"intents.json\"\n",
        "uploaded[file_name].decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"intents\": [\\r\\n    {\"tag\": \"greeting\",\\r\\n     \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\", \"Whats up\"],\\r\\n     \"responses\": [\"Hello!\", \"Good to see you again!\", \"Hi there, how can I help?\"],\\r\\n     \"context_set\": \"\"\\r\\n    },\\r\\n    {\"tag\": \"goodbye\",\\r\\n     \"patterns\": [\"cya\", \"See you later\", \"Goodbye\", \"I am Leaving\", \"Have a Good day\"],\\r\\n     \"responses\": [\"Sad to see you go :(\", \"Talk to you later\", \"Goodbye!\"],\\r\\n     \"context_set\": \"\"\\r\\n    },\\r\\n    {\"tag\": \"good\",\\r\\n     \"patterns\": [\"I am feeling great today\", \"feels good man\", \"Happy today\", \"Everything is going well\", \"Today is a good day\"],\\r\\n     \"responses\": [\"That\\'s great to hear! Keep pushing to reach your goals!\", \"Excellent! Get some work done today sir\"],\\r\\n     \"context_set\": \"\"\\r\\n    },\\r\\n    {\"tag\": \"bad\",\\r\\n     \"patterns\": [\"Today was not so great\", \"Not a good day today\", \"Feeling down\", \"No motivation today\", \"Depressed\", \"Sad\"],\\r\\n     \"responses\": [\"That\\'s alright, everyone has bad days, why not try exercising a bit\", \"You can do it! why not take a break and do something to refresh your mind\", \"Treat yourself to some food! Regain that motivation so you can do effective work!\"],\\r\\n     \"context_set\": \"\"\\r\\n    },\\r\\n    {\"tag\": \"okay\",\\r\\n     \"patterns\": [\"Today was okay\", \"today was fine, not the best, but not the worst\", \"Meh\", \"Today could be better\"],\\r\\n     \"responses\": [\"At least it\\'s not a bad day\", \"Why not make today even better by doing some Leetcode and improving your portfolio\", \"How about try learning something new\"],\\r\\n     \"context_set\": \"\"\\r\\n    },\\r\\n    {\"tag\": \"angry\",\\r\\n     \"patterns\": [\"I\\'m mad\", \"I am angry\", \"What the hell was today\", \"Today made me really really angry\", \"AHHHHH\", \"I WANT TO SCREAM\"],\\r\\n     \"responses\": [\"Calm down and lets work things out\", \"cool yourself, don\\'t let anger get the best of you\"],\\r\\n     \"context_set\": \"\"\\r\\n    }\\r\\n]\\r\\n}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bmu-WWOHWfQ"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBViTaklHZ2z"
      },
      "source": [
        "import codecs\n",
        "\n",
        "data=json.load(codecs.open('intents.json', 'r', 'utf-8-sig'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecbViJuXHeKV",
        "outputId": "9ab377ac-7dd6-4899-f4e1-882933958bc4"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'context_set': '',\n",
              "   'patterns': ['Hi',\n",
              "    'How are you',\n",
              "    'Is anyone there?',\n",
              "    'Hello',\n",
              "    'Good day',\n",
              "    'Whats up'],\n",
              "   'responses': ['Hello!',\n",
              "    'Good to see you again!',\n",
              "    'Hi there, how can I help?'],\n",
              "   'tag': 'greeting'},\n",
              "  {'context_set': '',\n",
              "   'patterns': ['cya',\n",
              "    'See you later',\n",
              "    'Goodbye',\n",
              "    'I am Leaving',\n",
              "    'Have a Good day'],\n",
              "   'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'],\n",
              "   'tag': 'goodbye'},\n",
              "  {'context_set': '',\n",
              "   'patterns': ['I am feeling great today',\n",
              "    'feels good man',\n",
              "    'Happy today',\n",
              "    'Everything is going well',\n",
              "    'Today is a good day'],\n",
              "   'responses': [\"That's great to hear! Keep pushing to reach your goals!\",\n",
              "    'Excellent! Get some work done today sir'],\n",
              "   'tag': 'good'},\n",
              "  {'context_set': '',\n",
              "   'patterns': ['Today was not so great',\n",
              "    'Not a good day today',\n",
              "    'Feeling down',\n",
              "    'No motivation today',\n",
              "    'Depressed',\n",
              "    'Sad'],\n",
              "   'responses': [\"That's alright, everyone has bad days, why not try exercising a bit\",\n",
              "    'You can do it! why not take a break and do something to refresh your mind',\n",
              "    'Treat yourself to some food! Regain that motivation so you can do effective work!'],\n",
              "   'tag': 'bad'},\n",
              "  {'context_set': '',\n",
              "   'patterns': ['Today was okay',\n",
              "    'today was fine, not the best, but not the worst',\n",
              "    'Meh',\n",
              "    'Today could be better'],\n",
              "   'responses': [\"At least it's not a bad day\",\n",
              "    'Why not make today even better by doing some Leetcode and improving your portfolio',\n",
              "    'How about try learning something new'],\n",
              "   'tag': 'okay'},\n",
              "  {'context_set': '',\n",
              "   'patterns': [\"I'm mad\",\n",
              "    'I am angry',\n",
              "    'What the hell was today',\n",
              "    'Today made me really really angry',\n",
              "    'AHHHHH',\n",
              "    'I WANT TO SCREAM'],\n",
              "   'responses': ['Calm down and lets work things out',\n",
              "    \"cool yourself, don't let anger get the best of you\"],\n",
              "   'tag': 'angry'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCyLSgHrHg7M"
      },
      "source": [
        "words = []\n",
        "labels =[]\n",
        "docs_x = []\n",
        "docs_y = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJlIAXOUHlf4",
        "outputId": "0b3f4952-3408-4e88-fb2f-d883f7f15393"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QWBKtfzHowS"
      },
      "source": [
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      #stemming\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        #addind the words in list\n",
        "        words.extend(wrds)\n",
        "        docs_x.append(wrds)\n",
        "        docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmYxaRXZHxhO"
      },
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "#set remove all the duplicate elements\n",
        "words = sorted(list(set(words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miEFDa-ZH0YB"
      },
      "source": [
        "labels = sorted(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ie21C6tH263"
      },
      "source": [
        "training = []\n",
        "output = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4fGI8DH6QO"
      },
      "source": [
        "out_empty = [0 for _ in range(len(labels))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw4hXu9CH9f0"
      },
      "source": [
        "for x, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "\n",
        "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "    for w in words:\n",
        "        if w in wrds:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq_9nWi4IA1L"
      },
      "source": [
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yKXwuGyIDxT"
      },
      "source": [
        "training = numpy.array(training)\n",
        "output = numpy.array(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pQkfD6xIGmf",
        "outputId": "3e81b5bb-2ddc-4cf0-9615-142effd51a31"
      },
      "source": [
        "#reseting all the previous settings\n",
        "tensorflow.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: GLPZ7Y\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
            "---------------------------------\n",
            "Training samples: 1\n",
            "Validation samples: 0\n",
            "--\n",
            "Training Step: 1  | time: 0.158s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m1.61260\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 002 | loss: 1.61260 - acc: 0.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.75775\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 003 | loss: 1.75775 - acc: 0.8182 -- iter: 1/1\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.78061\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 004 | loss: 1.78061 - acc: 0.9545 -- iter: 1/1\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.78464\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 005 | loss: 1.78464 - acc: 0.9860 -- iter: 1/1\n",
            "--\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.78461\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 006 | loss: 1.78461 - acc: 0.9950 -- iter: 1/1\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.78349\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 007 | loss: 1.78349 - acc: 0.9980 -- iter: 1/1\n",
            "--\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.78201\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 008 | loss: 1.78201 - acc: 0.9991 -- iter: 1/1\n",
            "--\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.78038\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 009 | loss: 1.78038 - acc: 0.9996 -- iter: 1/1\n",
            "--\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.77867\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 010 | loss: 1.77867 - acc: 0.9998 -- iter: 1/1\n",
            "--\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.77691\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 011 | loss: 1.77691 - acc: 0.9999 -- iter: 1/1\n",
            "--\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.77509\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 012 | loss: 1.77509 - acc: 0.9999 -- iter: 1/1\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.77323\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 013 | loss: 1.77323 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.77132\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 014 | loss: 1.77132 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.76936\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 015 | loss: 1.76936 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.76734\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 016 | loss: 1.76734 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.76527\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 017 | loss: 1.76527 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.76313\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 018 | loss: 1.76313 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.76092\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 019 | loss: 1.76092 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.75863\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 020 | loss: 1.75863 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.75627\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 021 | loss: 1.75627 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.75382\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 022 | loss: 1.75382 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.75127\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 023 | loss: 1.75127 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.74863\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 024 | loss: 1.74863 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.74589\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 025 | loss: 1.74589 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.74303\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 026 | loss: 1.74303 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.74006\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 027 | loss: 1.74006 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.73695\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 028 | loss: 1.73695 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.73372\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 029 | loss: 1.73372 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.73033\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 030 | loss: 1.73033 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.72680\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 031 | loss: 1.72680 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.72311\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 032 | loss: 1.72311 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.71925\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 033 | loss: 1.71925 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.71521\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 034 | loss: 1.71521 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.71098\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 035 | loss: 1.71098 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.70655\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 036 | loss: 1.70655 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.70192\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 037 | loss: 1.70192 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.69707\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 038 | loss: 1.69707 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.69200\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 039 | loss: 1.69200 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.68668\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 040 | loss: 1.68668 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.68112\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 041 | loss: 1.68112 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.67530\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 042 | loss: 1.67530 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.66921\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 043 | loss: 1.66921 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.66284\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 044 | loss: 1.66284 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.65617\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 045 | loss: 1.65617 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.64920\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 046 | loss: 1.64920 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.64192\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 047 | loss: 1.64192 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.63431\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 048 | loss: 1.63431 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.62637\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 049 | loss: 1.62637 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.61807\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 050 | loss: 1.61807 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.60942\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 051 | loss: 1.60942 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.60040\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 052 | loss: 1.60040 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.59100\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 053 | loss: 1.59100 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.58120\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 054 | loss: 1.58120 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.57100\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 055 | loss: 1.57100 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.56039\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 056 | loss: 1.56039 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.54936\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 057 | loss: 1.54936 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.53789\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 058 | loss: 1.53789 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.52598\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 059 | loss: 1.52598 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.51362\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 060 | loss: 1.51362 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.50081\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 061 | loss: 1.50081 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.48753\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 062 | loss: 1.48753 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.47377\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 063 | loss: 1.47377 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.45954\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 064 | loss: 1.45954 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.44482\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 065 | loss: 1.44482 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.42961\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 066 | loss: 1.42961 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.41390\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 067 | loss: 1.41390 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.39770\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 068 | loss: 1.39770 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.38101\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 069 | loss: 1.38101 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.34612\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 070 | loss: 1.34612 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.34612\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 071 | loss: 1.34612 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.32793\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 072 | loss: 1.32793 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.30924\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 073 | loss: 1.30924 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.29008\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 074 | loss: 1.29008 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.25031\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 075 | loss: 1.25031 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.25031\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 076 | loss: 1.25031 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.22973\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 077 | loss: 1.22973 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.20869\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 078 | loss: 1.20869 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.18723\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 079 | loss: 1.18723 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.16534\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 080 | loss: 1.16534 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.14306\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 081 | loss: 1.14306 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.12039\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 082 | loss: 1.12039 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.09712\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 083 | loss: 1.09712 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.07327\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 084 | loss: 1.07327 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.04889\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 085 | loss: 1.04889 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.02404\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 086 | loss: 1.02404 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.97308\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 087 | loss: 0.97308 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.97308\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 088 | loss: 0.97308 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.94708\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 089 | loss: 0.94708 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.92080\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 090 | loss: 0.92080 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.89429\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 091 | loss: 0.89429 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.86761\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 092 | loss: 0.86761 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.84081\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 093 | loss: 0.84081 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.81396\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 094 | loss: 0.81396 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.78711\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 095 | loss: 0.78711 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.76032\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 096 | loss: 0.76032 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.73364\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 097 | loss: 0.73364 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.70714\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 098 | loss: 0.70714 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.68087\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 099 | loss: 0.68087 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.65488\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 100 | loss: 0.65488 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.62924\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 101 | loss: 0.62924 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.60399\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 102 | loss: 0.60399 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.57917\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 103 | loss: 0.57917 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.55485\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 104 | loss: 0.55485 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.53104\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 105 | loss: 0.53104 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.50780\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 106 | loss: 0.50780 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.48517\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 107 | loss: 0.48517 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.46315\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 108 | loss: 0.46315 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.44180\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 109 | loss: 0.44180 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.42111\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 110 | loss: 0.42111 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.40112\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 111 | loss: 0.40112 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.38184\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 112 | loss: 0.38184 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.36326\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 113 | loss: 0.36326 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.34541\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 114 | loss: 0.34541 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.32827\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 115 | loss: 0.32827 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.31184\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 116 | loss: 0.31184 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.29613\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 117 | loss: 0.29613 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.28111\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 118 | loss: 0.28111 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.26679\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 119 | loss: 0.26679 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.25313\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 120 | loss: 0.25313 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.24014\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 121 | loss: 0.24014 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.22779\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 122 | loss: 0.22779 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.21606\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 123 | loss: 0.21606 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.20493\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 124 | loss: 0.20493 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.19438\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 125 | loss: 0.19438 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.18439\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 126 | loss: 0.18439 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.17493\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 127 | loss: 0.17493 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.16600\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 128 | loss: 0.16600 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.15755\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 129 | loss: 0.15755 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.14957\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 130 | loss: 0.14957 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.14204\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 131 | loss: 0.14204 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.13494\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 132 | loss: 0.13494 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.12824\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 133 | loss: 0.12824 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.12193\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 134 | loss: 0.12193 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.11598\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 135 | loss: 0.11598 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.11037\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 136 | loss: 0.11037 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.10509\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 137 | loss: 0.10509 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.10012\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 138 | loss: 0.10012 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.09543\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 139 | loss: 0.09543 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.09102\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 140 | loss: 0.09102 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.08296\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 141 | loss: 0.08296 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.08296\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 142 | loss: 0.08296 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.07928\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 143 | loss: 0.07928 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.07582\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 144 | loss: 0.07582 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.07255\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 145 | loss: 0.07255 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.06948\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 146 | loss: 0.06948 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.06658\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 147 | loss: 0.06658 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.06385\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 148 | loss: 0.06385 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.06127\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 149 | loss: 0.06127 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.05884\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 150 | loss: 0.05884 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.05656\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 151 | loss: 0.05656 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.05439\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 152 | loss: 0.05439 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.05235\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 153 | loss: 0.05235 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.05043\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 154 | loss: 0.05043 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.04861\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 155 | loss: 0.04861 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.04689\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 156 | loss: 0.04689 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.04526\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 157 | loss: 0.04526 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.04372\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 158 | loss: 0.04372 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.04226\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 159 | loss: 0.04226 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.04088\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 160 | loss: 0.04088 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.03957\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 161 | loss: 0.03957 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.03833\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 162 | loss: 0.03833 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.03715\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 163 | loss: 0.03715 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.03603\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 164 | loss: 0.03603 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.03497\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 165 | loss: 0.03497 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.03396\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 166 | loss: 0.03396 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.03300\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 167 | loss: 0.03300 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.03208\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 168 | loss: 0.03208 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.03121\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 169 | loss: 0.03121 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.03038\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 170 | loss: 0.03038 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.02959\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 171 | loss: 0.02959 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.02883\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 172 | loss: 0.02883 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.02811\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 173 | loss: 0.02811 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.02742\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 174 | loss: 0.02742 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.02675\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 175 | loss: 0.02675 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.02612\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 176 | loss: 0.02612 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.02551\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 177 | loss: 0.02551 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 178 | loss: 0.02493 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.02438\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 179 | loss: 0.02438 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.02384\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 180 | loss: 0.02384 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 181 | loss: 0.02333 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 182 | loss: 0.02283 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.02236\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 183 | loss: 0.02236 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.02190\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 184 | loss: 0.02190 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.02103\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 185 | loss: 0.02103 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.02103\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 186 | loss: 0.02103 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 187 | loss: 0.02062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 188 | loss: 0.02023 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 189 | loss: 0.01985 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 190 | loss: 0.01948 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 191 | loss: 0.01912 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 192 | loss: 0.01878 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 193 | loss: 0.01844 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 194 | loss: 0.01812 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 195 | loss: 0.01781 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 196 | loss: 0.01751 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 197 | loss: 0.01721 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.01693\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 198 | loss: 0.01693 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.01665\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 199 | loss: 0.01665 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.01638\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 200 | loss: 0.01638 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.01612\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 201 | loss: 0.01612 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.01587\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 202 | loss: 0.01587 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 203 | loss: 0.01562 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.01538\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 204 | loss: 0.01538 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 205 | loss: 0.01515 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.01492\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 206 | loss: 0.01492 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 207 | loss: 0.01470 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.01449\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 208 | loss: 0.01449 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.01428\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 209 | loss: 0.01428 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.01407\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 210 | loss: 0.01407 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.01387\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 211 | loss: 0.01387 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.01368\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 212 | loss: 0.01368 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.01349\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 213 | loss: 0.01349 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.01330\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 214 | loss: 0.01330 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.01312\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 215 | loss: 0.01312 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.01295\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 216 | loss: 0.01295 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.01277\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 217 | loss: 0.01277 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.01260\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 218 | loss: 0.01260 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.01244\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 219 | loss: 0.01244 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.01228\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 220 | loss: 0.01228 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.01212\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 221 | loss: 0.01212 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.01197\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 222 | loss: 0.01197 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.01181\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 223 | loss: 0.01181 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.01167\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 224 | loss: 0.01167 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.01152\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 225 | loss: 0.01152 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.01138\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 226 | loss: 0.01138 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.01124\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 227 | loss: 0.01124 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.01111\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 228 | loss: 0.01111 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.01097\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 229 | loss: 0.01097 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.01084\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 230 | loss: 0.01084 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.01071\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 231 | loss: 0.01071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.01059\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 232 | loss: 0.01059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.01046\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 233 | loss: 0.01046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.01034\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 234 | loss: 0.01034 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.01023\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 235 | loss: 0.01023 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.01011\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 236 | loss: 0.01011 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.01000\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 237 | loss: 0.01000 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.00988\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 238 | loss: 0.00988 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.00977\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 239 | loss: 0.00977 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.00967\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 240 | loss: 0.00967 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.00956\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 241 | loss: 0.00956 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.00946\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 242 | loss: 0.00946 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.00935\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 243 | loss: 0.00935 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.00925\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 244 | loss: 0.00925 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.00915\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 245 | loss: 0.00915 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.00906\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 246 | loss: 0.00906 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.00896\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 247 | loss: 0.00896 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.00887\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 248 | loss: 0.00887 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.00878\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 249 | loss: 0.00878 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.00869\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 250 | loss: 0.00869 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.00860\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 251 | loss: 0.00860 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.00851\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 252 | loss: 0.00851 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.00842\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 253 | loss: 0.00842 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.00834\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 254 | loss: 0.00834 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.00826\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 255 | loss: 0.00826 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.00817\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 256 | loss: 0.00817 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.00809\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 257 | loss: 0.00809 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.00801\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 258 | loss: 0.00801 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.00794\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 259 | loss: 0.00794 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.00786\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 260 | loss: 0.00786 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.00778\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 261 | loss: 0.00778 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.00771\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 262 | loss: 0.00771 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.00764\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 263 | loss: 0.00764 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.00756\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 264 | loss: 0.00756 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.00749\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 265 | loss: 0.00749 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.00742\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 266 | loss: 0.00742 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.00735\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 267 | loss: 0.00735 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.00729\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 268 | loss: 0.00729 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.00722\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 269 | loss: 0.00722 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.00715\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 270 | loss: 0.00715 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.00709\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 271 | loss: 0.00709 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.00702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 272 | loss: 0.00702 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.00696\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 273 | loss: 0.00696 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.00690\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 274 | loss: 0.00690 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.00684\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 275 | loss: 0.00684 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.00678\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 276 | loss: 0.00678 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.00672\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 277 | loss: 0.00672 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.00666\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 278 | loss: 0.00666 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.00660\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 279 | loss: 0.00660 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.00654\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 280 | loss: 0.00654 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.00649\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 281 | loss: 0.00649 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.00643\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 282 | loss: 0.00643 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.00638\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 283 | loss: 0.00638 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.00632\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 284 | loss: 0.00632 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.00627\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 285 | loss: 0.00627 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.00622\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 286 | loss: 0.00622 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.00616\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 287 | loss: 0.00616 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.00611\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 288 | loss: 0.00611 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.00606\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 289 | loss: 0.00606 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.00601\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 290 | loss: 0.00601 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.00596\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 291 | loss: 0.00596 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.00591\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 292 | loss: 0.00591 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.00587\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 293 | loss: 0.00587 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.00582\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 294 | loss: 0.00582 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.00577\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 295 | loss: 0.00577 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.00573\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 296 | loss: 0.00573 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.00568\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 297 | loss: 0.00568 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.00564\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 298 | loss: 0.00564 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.00559\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 299 | loss: 0.00559 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.00555\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 300 | loss: 0.00555 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.00550\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 301 | loss: 0.00550 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.00546\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 302 | loss: 0.00546 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.00542\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 303 | loss: 0.00542 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.00534\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 304 | loss: 0.00534 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.00534\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 305 | loss: 0.00534 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.00529\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 306 | loss: 0.00529 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.00525\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 307 | loss: 0.00525 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.00521\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 308 | loss: 0.00521 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.00518\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 309 | loss: 0.00518 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.00514\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 310 | loss: 0.00514 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.00510\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 311 | loss: 0.00510 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.00506\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 312 | loss: 0.00506 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.00502\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 313 | loss: 0.00502 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.00499\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 314 | loss: 0.00499 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.00495\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 315 | loss: 0.00495 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.00491\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 316 | loss: 0.00491 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.00488\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 317 | loss: 0.00488 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.00484\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 318 | loss: 0.00484 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.00481\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 319 | loss: 0.00481 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.00477\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 320 | loss: 0.00477 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.00474\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 321 | loss: 0.00474 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.00470\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 322 | loss: 0.00470 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.00467\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 323 | loss: 0.00467 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.00464\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 324 | loss: 0.00464 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.00460\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 325 | loss: 0.00460 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.00457\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 326 | loss: 0.00457 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.00454\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 327 | loss: 0.00454 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.00451\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 328 | loss: 0.00451 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.00448\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 329 | loss: 0.00448 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.00445\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 330 | loss: 0.00445 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.00442\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 331 | loss: 0.00442 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.00439\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 332 | loss: 0.00439 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.00436\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 333 | loss: 0.00436 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.00433\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 334 | loss: 0.00433 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.00430\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 335 | loss: 0.00430 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.00427\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 336 | loss: 0.00427 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.00424\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 337 | loss: 0.00424 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.00421\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 338 | loss: 0.00421 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.00418\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 339 | loss: 0.00418 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.00415\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 340 | loss: 0.00415 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.00413\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 341 | loss: 0.00413 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.00410\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 342 | loss: 0.00410 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.00407\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 343 | loss: 0.00407 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.00405\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 344 | loss: 0.00405 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.00402\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 345 | loss: 0.00402 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.00399\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 346 | loss: 0.00399 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.00397\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 347 | loss: 0.00397 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.00394\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 348 | loss: 0.00394 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.00392\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 349 | loss: 0.00392 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.00389\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 350 | loss: 0.00389 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.00387\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 351 | loss: 0.00387 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.00384\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 352 | loss: 0.00384 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.00382\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 353 | loss: 0.00382 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.00379\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 354 | loss: 0.00379 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.00377\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 355 | loss: 0.00377 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.00375\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 356 | loss: 0.00375 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.00372\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 357 | loss: 0.00372 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.00370\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 358 | loss: 0.00370 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.00368\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 359 | loss: 0.00368 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.00365\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 360 | loss: 0.00365 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.00363\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 361 | loss: 0.00363 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.00361\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 362 | loss: 0.00361 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.00359\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 363 | loss: 0.00359 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.00357\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 364 | loss: 0.00357 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.00354\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 365 | loss: 0.00354 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.00352\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 366 | loss: 0.00352 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.00350\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 367 | loss: 0.00350 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.00348\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 368 | loss: 0.00348 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.00346\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 369 | loss: 0.00346 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.00344\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 370 | loss: 0.00344 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.00342\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 371 | loss: 0.00342 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.00340\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 372 | loss: 0.00340 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.00338\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 373 | loss: 0.00338 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.00336\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 374 | loss: 0.00336 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.00334\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 375 | loss: 0.00334 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.00332\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 376 | loss: 0.00332 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.00330\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 377 | loss: 0.00330 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.00328\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 378 | loss: 0.00328 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.00326\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 379 | loss: 0.00326 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.00324\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 380 | loss: 0.00324 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.00322\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 381 | loss: 0.00322 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.00321\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 382 | loss: 0.00321 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.00319\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 383 | loss: 0.00319 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 384 | loss: 0.00317 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.00315\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 385 | loss: 0.00315 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.00313\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 386 | loss: 0.00313 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.00312\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 387 | loss: 0.00312 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 388 | loss: 0.00310 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.00308\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 389 | loss: 0.00308 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.00306\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 390 | loss: 0.00306 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.00305\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 391 | loss: 0.00305 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.00303\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 392 | loss: 0.00303 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.00301\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 393 | loss: 0.00301 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 394 | loss: 0.00300 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 395 | loss: 0.00298 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.00296\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 396 | loss: 0.00296 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 397 | loss: 0.00295 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 398 | loss: 0.00293 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.00291\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 399 | loss: 0.00291 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 400 | loss: 0.00290 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.00288\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 401 | loss: 0.00288 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 402 | loss: 0.00287 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 403 | loss: 0.00285 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 404 | loss: 0.00284 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.00282\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 405 | loss: 0.00282 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 406 | loss: 0.00281 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.00279\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 407 | loss: 0.00279 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.00278\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 408 | loss: 0.00278 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.00276\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 409 | loss: 0.00276 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.00275\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 410 | loss: 0.00275 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.00273\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 411 | loss: 0.00273 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.00272\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 412 | loss: 0.00272 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.00271\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 413 | loss: 0.00271 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 414 | loss: 0.00269 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.00268\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 415 | loss: 0.00268 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.00266\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 416 | loss: 0.00266 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.00265\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 417 | loss: 0.00265 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.00264\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 418 | loss: 0.00264 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.00262\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 419 | loss: 0.00262 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.00261\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 420 | loss: 0.00261 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.00260\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 421 | loss: 0.00260 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.00258\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 422 | loss: 0.00258 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.00257\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 423 | loss: 0.00257 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.00256\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 424 | loss: 0.00256 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.00254\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 425 | loss: 0.00254 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.00253\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 426 | loss: 0.00253 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.00252\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 427 | loss: 0.00252 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.00251\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 428 | loss: 0.00251 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.00249\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 429 | loss: 0.00249 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.00248\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 430 | loss: 0.00248 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.00247\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 431 | loss: 0.00247 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.00246\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 432 | loss: 0.00246 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.00244\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 433 | loss: 0.00244 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.00243\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 434 | loss: 0.00243 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.00242\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 435 | loss: 0.00242 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.00241\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 436 | loss: 0.00241 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.00240\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 437 | loss: 0.00240 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.00238\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 438 | loss: 0.00238 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.00237\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 439 | loss: 0.00237 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.00236\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 440 | loss: 0.00236 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.00235\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 441 | loss: 0.00235 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.00234\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 442 | loss: 0.00234 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.00233\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 443 | loss: 0.00233 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.00232\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 444 | loss: 0.00232 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.00231\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 445 | loss: 0.00231 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.00229\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 446 | loss: 0.00229 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.00228\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 447 | loss: 0.00228 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.00227\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 448 | loss: 0.00227 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.00226\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 449 | loss: 0.00226 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.00225\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 450 | loss: 0.00225 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.00224\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 451 | loss: 0.00224 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.00223\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 452 | loss: 0.00223 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.00222\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 453 | loss: 0.00222 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.00221\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 454 | loss: 0.00221 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.00220\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 455 | loss: 0.00220 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.00219\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 456 | loss: 0.00219 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.00218\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 457 | loss: 0.00218 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.00217\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 458 | loss: 0.00217 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.00216\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 459 | loss: 0.00216 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.00215\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 460 | loss: 0.00215 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.00214\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 461 | loss: 0.00214 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.00213\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 462 | loss: 0.00213 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.00212\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 463 | loss: 0.00212 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.00211\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 464 | loss: 0.00211 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.00210\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 465 | loss: 0.00210 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.00209\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 466 | loss: 0.00209 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.00208\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 467 | loss: 0.00208 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.00207\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 468 | loss: 0.00207 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.00206\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 469 | loss: 0.00206 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.00205\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 470 | loss: 0.00205 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.00204\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 471 | loss: 0.00204 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.00203\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 472 | loss: 0.00203 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.00203\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 473 | loss: 0.00203 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.00202\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 474 | loss: 0.00202 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.00201\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 475 | loss: 0.00201 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.00200\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 476 | loss: 0.00200 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.00199\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 477 | loss: 0.00199 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.00198\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 478 | loss: 0.00198 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.00197\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 479 | loss: 0.00197 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.00196\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 480 | loss: 0.00196 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.00195\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 481 | loss: 0.00195 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.00195\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 482 | loss: 0.00195 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.00194\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 483 | loss: 0.00194 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.00193\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 484 | loss: 0.00193 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.00192\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 485 | loss: 0.00192 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.00191\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 486 | loss: 0.00191 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.00190\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 487 | loss: 0.00190 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.00190\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 488 | loss: 0.00190 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.00189\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 489 | loss: 0.00189 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.00188\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 490 | loss: 0.00188 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.00187\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 491 | loss: 0.00187 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.00186\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 492 | loss: 0.00186 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.00186\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 493 | loss: 0.00186 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.00185\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 494 | loss: 0.00185 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.00184\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 495 | loss: 0.00184 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.00183\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 496 | loss: 0.00183 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.00183\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 497 | loss: 0.00183 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.00182\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 498 | loss: 0.00182 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.00181\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 499 | loss: 0.00181 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.00180\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 500 | loss: 0.00180 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.00179\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 501 | loss: 0.00179 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.00179\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 502 | loss: 0.00179 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.00178\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 503 | loss: 0.00178 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.00177\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 504 | loss: 0.00177 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.00177\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 505 | loss: 0.00177 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.00176\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 506 | loss: 0.00176 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.00175\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 507 | loss: 0.00175 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.00174\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 508 | loss: 0.00174 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.00174\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 509 | loss: 0.00174 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.00173\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 510 | loss: 0.00173 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.00172\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 511 | loss: 0.00172 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.00172\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 512 | loss: 0.00172 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.00171\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 513 | loss: 0.00171 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.00170\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 514 | loss: 0.00170 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.00169\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 515 | loss: 0.00169 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.00169\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 516 | loss: 0.00169 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.00168\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 517 | loss: 0.00168 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.00167\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 518 | loss: 0.00167 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.00167\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 519 | loss: 0.00167 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.00166\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 520 | loss: 0.00166 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.00165\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 521 | loss: 0.00165 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.00165\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 522 | loss: 0.00165 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.00164\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 523 | loss: 0.00164 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.00163\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 524 | loss: 0.00163 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.00163\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 525 | loss: 0.00163 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.00162\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 526 | loss: 0.00162 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.00161\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 527 | loss: 0.00161 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.00161\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 528 | loss: 0.00161 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.00160\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 529 | loss: 0.00160 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.00160\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 530 | loss: 0.00160 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.00159\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 531 | loss: 0.00159 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.00158\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 532 | loss: 0.00158 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.00158\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 533 | loss: 0.00158 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.00157\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 534 | loss: 0.00157 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.00156\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 535 | loss: 0.00156 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.00156\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 536 | loss: 0.00156 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.00155\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 537 | loss: 0.00155 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.00155\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 538 | loss: 0.00155 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.00154\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 539 | loss: 0.00154 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.00153\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 540 | loss: 0.00153 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.00153\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 541 | loss: 0.00153 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.00152\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 542 | loss: 0.00152 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.00152\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 543 | loss: 0.00152 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.00151\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 544 | loss: 0.00151 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.00151\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 545 | loss: 0.00151 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.00150\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 546 | loss: 0.00150 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.00149\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 547 | loss: 0.00149 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.00149\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 548 | loss: 0.00149 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.00148\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 549 | loss: 0.00148 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.00148\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 550 | loss: 0.00148 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.00147\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 551 | loss: 0.00147 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.00147\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 552 | loss: 0.00147 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.00146\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 553 | loss: 0.00146 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.00146\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 554 | loss: 0.00146 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.00145\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 555 | loss: 0.00145 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.00144\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 556 | loss: 0.00144 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.00144\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 557 | loss: 0.00144 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.00143\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 558 | loss: 0.00143 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.00143\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 559 | loss: 0.00143 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.00142\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 560 | loss: 0.00142 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.00142\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 561 | loss: 0.00142 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.00141\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 562 | loss: 0.00141 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.00141\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 563 | loss: 0.00141 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.00140\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 564 | loss: 0.00140 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.00140\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 565 | loss: 0.00140 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.00139\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 566 | loss: 0.00139 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.00139\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 567 | loss: 0.00139 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.00138\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 568 | loss: 0.00138 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.00138\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 569 | loss: 0.00138 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.00137\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 570 | loss: 0.00137 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.00137\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 571 | loss: 0.00137 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.00136\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 572 | loss: 0.00136 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.00136\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 573 | loss: 0.00136 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.00135\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 574 | loss: 0.00135 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.00135\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 575 | loss: 0.00135 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.00134\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 576 | loss: 0.00134 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.00134\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 577 | loss: 0.00134 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.00133\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 578 | loss: 0.00133 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.00133\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 579 | loss: 0.00133 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.00132\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 580 | loss: 0.00132 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.00132\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 581 | loss: 0.00132 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.00131\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 582 | loss: 0.00131 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.00131\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 583 | loss: 0.00131 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.00131\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 584 | loss: 0.00131 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.00130\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 585 | loss: 0.00130 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.00130\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 586 | loss: 0.00130 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.00129\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 587 | loss: 0.00129 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.00129\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 588 | loss: 0.00129 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.00128\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 589 | loss: 0.00128 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.00128\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 590 | loss: 0.00128 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.00127\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 591 | loss: 0.00127 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.00127\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 592 | loss: 0.00127 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.00126\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 593 | loss: 0.00126 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.00126\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 594 | loss: 0.00126 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.00126\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 595 | loss: 0.00126 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.00125\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 596 | loss: 0.00125 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.00125\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 597 | loss: 0.00125 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.00124\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 598 | loss: 0.00124 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.00124\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 599 | loss: 0.00124 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.00123\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 600 | loss: 0.00123 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.00123\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 601 | loss: 0.00123 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.00123\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 602 | loss: 0.00123 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.00122\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 603 | loss: 0.00122 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.00122\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 604 | loss: 0.00122 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.00121\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 605 | loss: 0.00121 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.00121\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 606 | loss: 0.00121 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.00121\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 607 | loss: 0.00121 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.00120\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 608 | loss: 0.00120 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.00120\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 609 | loss: 0.00120 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.00119\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 610 | loss: 0.00119 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.00119\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 611 | loss: 0.00119 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.00119\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 612 | loss: 0.00119 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.00118\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 613 | loss: 0.00118 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.00118\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 614 | loss: 0.00118 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.00117\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 615 | loss: 0.00117 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.00117\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 616 | loss: 0.00117 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.00117\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 617 | loss: 0.00117 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.00116\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 618 | loss: 0.00116 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.00116\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 619 | loss: 0.00116 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.00115\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 620 | loss: 0.00115 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.00115\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 621 | loss: 0.00115 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.00115\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 622 | loss: 0.00115 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.00114\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 623 | loss: 0.00114 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.00114\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 624 | loss: 0.00114 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.00114\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 625 | loss: 0.00114 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.00113\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 626 | loss: 0.00113 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.00113\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 627 | loss: 0.00113 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.00112\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 628 | loss: 0.00112 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.00112\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 629 | loss: 0.00112 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.00112\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 630 | loss: 0.00112 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.00111\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 631 | loss: 0.00111 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.00111\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 632 | loss: 0.00111 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.00111\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 633 | loss: 0.00111 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.00110\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 634 | loss: 0.00110 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.00110\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 635 | loss: 0.00110 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.00110\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 636 | loss: 0.00110 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.00109\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 637 | loss: 0.00109 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.00109\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 638 | loss: 0.00109 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.00108\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 639 | loss: 0.00108 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.00108\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 640 | loss: 0.00108 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.00108\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 641 | loss: 0.00108 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.00107\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 642 | loss: 0.00107 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.00107\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 643 | loss: 0.00107 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.00107\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 644 | loss: 0.00107 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.00106\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 645 | loss: 0.00106 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.00106\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 646 | loss: 0.00106 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.00106\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 647 | loss: 0.00106 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.00105\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 648 | loss: 0.00105 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.00105\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 649 | loss: 0.00105 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.00105\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 650 | loss: 0.00105 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.00104\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 651 | loss: 0.00104 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.00104\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 652 | loss: 0.00104 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.00104\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 653 | loss: 0.00104 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.00103\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 654 | loss: 0.00103 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.00103\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 655 | loss: 0.00103 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.00103\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 656 | loss: 0.00103 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.00102\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 657 | loss: 0.00102 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.00102\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 658 | loss: 0.00102 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.00102\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 659 | loss: 0.00102 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.00102\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 660 | loss: 0.00102 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.00101\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 661 | loss: 0.00101 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.00101\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 662 | loss: 0.00101 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.00101\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 663 | loss: 0.00101 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.00100\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 664 | loss: 0.00100 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.00100\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 665 | loss: 0.00100 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.00100\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 666 | loss: 0.00100 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.00099\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 667 | loss: 0.00099 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.00099\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 668 | loss: 0.00099 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.00099\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 669 | loss: 0.00099 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 670 | loss: 0.00098 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 671 | loss: 0.00098 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 672 | loss: 0.00098 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 673 | loss: 0.00098 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.00097\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 674 | loss: 0.00097 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.00097\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 675 | loss: 0.00097 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.00097\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 676 | loss: 0.00097 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 677 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 678 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 679 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 680 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.00095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 681 | loss: 0.00095 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.00095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 682 | loss: 0.00095 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.00095\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 683 | loss: 0.00095 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 684 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 685 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 686 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 687 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.00093\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 688 | loss: 0.00093 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.00093\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 689 | loss: 0.00093 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.00093\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 690 | loss: 0.00093 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 691 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 692 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 693 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 694 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.00091\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 695 | loss: 0.00091 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.00091\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 696 | loss: 0.00091 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.00091\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 697 | loss: 0.00091 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.00091\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 698 | loss: 0.00091 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 699 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 700 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 701 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 702 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 703 | loss: 0.00089 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 704 | loss: 0.00089 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 705 | loss: 0.00089 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.00088\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 706 | loss: 0.00088 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.00088\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 707 | loss: 0.00088 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.00088\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 708 | loss: 0.00088 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.00088\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 709 | loss: 0.00088 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 710 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 711 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 712 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 713 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 714 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 715 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 716 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 717 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 718 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 719 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 720 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 721 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 722 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 723 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 724 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 725 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 726 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 727 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 728 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 729 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 730 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 731 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 732 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 733 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 734 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 735 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 736 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 737 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 738 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 739 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 740 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 741 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 742 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 743 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 744 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 745 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 746 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 747 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 748 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 749 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 750 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 751 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 752 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 753 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.00077\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 754 | loss: 0.00077 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.00077\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 755 | loss: 0.00077 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.00077\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 756 | loss: 0.00077 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.00077\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 757 | loss: 0.00077 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.00077\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 758 | loss: 0.00077 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.00076\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 759 | loss: 0.00076 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.00076\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 760 | loss: 0.00076 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.00076\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 761 | loss: 0.00076 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.00076\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 762 | loss: 0.00076 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.00076\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 763 | loss: 0.00076 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.00075\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 764 | loss: 0.00075 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.00075\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 765 | loss: 0.00075 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.00075\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 766 | loss: 0.00075 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.00075\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 767 | loss: 0.00075 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.00075\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 768 | loss: 0.00075 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.00074\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 769 | loss: 0.00074 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.00074\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 770 | loss: 0.00074 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.00074\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 771 | loss: 0.00074 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.00074\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 772 | loss: 0.00074 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.00074\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 773 | loss: 0.00074 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.00073\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 774 | loss: 0.00073 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.00073\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 775 | loss: 0.00073 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.00073\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 776 | loss: 0.00073 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.00073\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 777 | loss: 0.00073 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.00073\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 778 | loss: 0.00073 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.00072\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 779 | loss: 0.00072 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.00072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 780 | loss: 0.00072 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.00072\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 781 | loss: 0.00072 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.00072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 782 | loss: 0.00072 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.00072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 783 | loss: 0.00072 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.00071\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 784 | loss: 0.00071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.00071\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 785 | loss: 0.00071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.00071\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 786 | loss: 0.00071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.00071\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 787 | loss: 0.00071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.00071\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 788 | loss: 0.00071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.00071\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 789 | loss: 0.00071 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.00070\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 790 | loss: 0.00070 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.00070\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 791 | loss: 0.00070 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.00070\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 792 | loss: 0.00070 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.00070\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 793 | loss: 0.00070 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.00070\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 794 | loss: 0.00070 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.00069\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 795 | loss: 0.00069 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.00069\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 796 | loss: 0.00069 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.00069\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 797 | loss: 0.00069 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.00069\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 798 | loss: 0.00069 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.00069\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 799 | loss: 0.00069 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.00069\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 800 | loss: 0.00069 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 801 | loss: 0.00068 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 802 | loss: 0.00068 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 803 | loss: 0.00068 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 804 | loss: 0.00068 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 805 | loss: 0.00068 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 806 | loss: 0.00068 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.00067\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 807 | loss: 0.00067 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.00067\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 808 | loss: 0.00067 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.00067\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 809 | loss: 0.00067 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.00067\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 810 | loss: 0.00067 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.00067\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 811 | loss: 0.00067 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.00067\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 812 | loss: 0.00067 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.00066\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 813 | loss: 0.00066 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.00066\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 814 | loss: 0.00066 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.00066\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 815 | loss: 0.00066 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.00066\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 816 | loss: 0.00066 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.00066\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 817 | loss: 0.00066 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.00066\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 818 | loss: 0.00066 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.00065\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 819 | loss: 0.00065 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.00065\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 820 | loss: 0.00065 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.00065\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 821 | loss: 0.00065 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.00065\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 822 | loss: 0.00065 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.00065\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 823 | loss: 0.00065 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.00065\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 824 | loss: 0.00065 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.00064\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 825 | loss: 0.00064 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.00064\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 826 | loss: 0.00064 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.00064\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 827 | loss: 0.00064 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.00064\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 828 | loss: 0.00064 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.00064\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 829 | loss: 0.00064 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.00064\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 830 | loss: 0.00064 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 831 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 832 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 833 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 834 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 835 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 836 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 837 | loss: 0.00063 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 838 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 839 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 840 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 841 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 842 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 843 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 844 | loss: 0.00062 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.00061\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 845 | loss: 0.00061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.00061\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 846 | loss: 0.00061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.00061\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 847 | loss: 0.00061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.00061\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 848 | loss: 0.00061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.00061\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 849 | loss: 0.00061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.00061\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 850 | loss: 0.00061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 851 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 852 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 853 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 854 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 855 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 856 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.00060\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 857 | loss: 0.00060 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 858 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 859 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 860 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 861 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 862 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 863 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 864 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.00059\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 865 | loss: 0.00059 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 866 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 867 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 868 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 869 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 870 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 871 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.00058\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 872 | loss: 0.00058 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 873 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 874 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 875 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 876 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 877 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 878 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 879 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 880 | loss: 0.00057 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 881 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 882 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 883 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 884 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 885 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 886 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.00056\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 887 | loss: 0.00056 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 888 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 889 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 890 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 891 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 892 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 893 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 894 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.00055\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 895 | loss: 0.00055 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 896 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 897 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 898 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 899 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 900 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 901 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 902 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.00054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 903 | loss: 0.00054 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 904 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 905 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 906 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 907 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 908 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 909 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 910 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 911 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 912 | loss: 0.00053 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 913 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 914 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 915 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 916 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 917 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 918 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 919 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 920 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.00052\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 921 | loss: 0.00052 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 922 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 923 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 924 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 925 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 926 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 927 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 928 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.00051\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 929 | loss: 0.00051 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 930 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 931 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 932 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 933 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 934 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 935 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 936 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 937 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 938 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 939 | loss: 0.00050 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 940 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 941 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 942 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 943 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 944 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 945 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 946 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 947 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.00049\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 948 | loss: 0.00049 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 949 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 950 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 951 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 952 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 953 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 954 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 955 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 956 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 957 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.00048\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 958 | loss: 0.00048 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 959 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 960 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 961 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 962 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 963 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 964 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 965 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 966 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 967 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 968 | loss: 0.00047 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 969 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 970 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 971 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 972 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 973 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 974 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 975 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 976 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 977 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 978 | loss: 0.00046 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 979 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 980 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 981 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 982 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 983 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 984 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 985 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 986 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 987 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 988 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.00045\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 989 | loss: 0.00045 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 990 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 991 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 992 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 993 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 994 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 995 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 996 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 997 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 998 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 999 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.00044\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1000 | loss: 0.00044 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKUhitvgJ-wB",
        "outputId": "d129c5e4-65b7-49b4-a76a-79eb078bfc02"
      },
      "source": [
        "try:\n",
        "    model.load(\"model.tflearn\")\n",
        "except:\n",
        "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "    model.save(\"model.tflearn\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtBSTF5HKHBn"
      },
      "source": [
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "    \n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "            \n",
        "    return numpy.array(bag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXk1ozGOKLBa"
      },
      "source": [
        "def chat():\n",
        "    print(\"Start talking with the bot (type quit to stop)!\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        results = model.predict([bag_of_words(inp, words)])\n",
        "        results_index = numpy.argmax(results)\n",
        "        tag = labels[results_index]\n",
        "\n",
        "        for tg in data[\"intents\"]:\n",
        "            if tg['tag'] == tag:\n",
        "                responses = tg['responses']\n",
        "\n",
        "        print(random.choice(responses))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}